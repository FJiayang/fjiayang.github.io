<html>

<head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>
    Kubernetes | F嘉阳
</title>
<link rel="shortcut icon" href="https://fjiayang.github.io//favicon.ico?v=1575360921810">
<!-- <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous"> -->
<link href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://fjiayang.github.io//styles/main.css">
<!-- js -->
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="https://fjiayang.github.io//media/js/jquery.sticky-sidebar.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/moment.js/2.23.0/moment.min.js"></script>


</head>

<body>
    <div class="main">
        <div class="header">
    <div class="nav">
        <div class="logo">
            <a href="https://fjiayang.github.io/">
                <img class="avatar" src="https://fjiayang.github.io//images/avatar.png?v=1575360921810" alt="">
            </a>
            <div class="site-title">
                <h1>
                    F嘉阳
                </h1>
            </div>
        </div>
        <span class="menu-btn fa fa-align-justify"></span>
        <div class="menu-container">
            <ul>
                
                    
                            <li>
                                <a href="/" class="menu">
                                    首页
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/archives" class="menu">
                                    归档
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/tags" class="menu">
                                    标签
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/post/about" class="menu">
                                    关于
                                </a>
                            </li>
                            
                                
            </ul>
        </div>
    </div>
</div>

<script>
    $(document).ready(function() {
        $(".menu-btn").click(function() {
            $(".menu-container").slideToggle();
        });
        $(window).resize(function() {

            if (window.matchMedia('(min-width: 960px)').matches) {
                $(".menu-container").css('display', 'block')
            } else {
                $(".menu-container").css('display', 'none')
            }

        });
    });
</script>

            <div id="main-content" class="post-container main-container">
                <div id="content" class="main-container-left">
                    
    <div class="i-card">
        <b>标签：#
        Kubernetes</b>
    </div>
    
        
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://fjiayang.github.io//post/kubernetes-ji-chu-ming-ling-xue-xi">
                        Kubernetes基础命令学习
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2019-10-20</time>
                    
                        <a href="https://fjiayang.github.io//tag/VwGnvPo1M" class="post-tag i-tag
                            i-tag-other_1">
            #学习
        </a>
                        
                        <a href="https://fjiayang.github.io//tag/yb4HlP89R" class="post-tag i-tag
                            i-tag-warning">
            #Kubernetes
        </a>
                        
                </div>
                <div class="post-article">
                    
                        <a href="https://fjiayang.github.io//post/kubernetes-ji-chu-ming-ling-xue-xi" class="post-feature-image" style="background-image:url(https://s2.ax1x.com/2019/11/27/QC2UuF.png) ">
                        </a>
                        
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            配置命令语法提示
执行命令查看指南
$ kubectl completion -h
Output shell completion code for the specified shell (bash or zsh). The shell code must be evaluated to provide
interactive completion of kubectl commands.  This can be done by sourcing it from the .bash_profile.

 Detailed instructions on how to do this are available here:
https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion

 Note for zsh users: [1] zsh completions are only supported in versions of zsh &amp;gt;= 5.2

Examples:
  # Installing bash completion on macOS using homebrew
  ## If running Bash 3.2 included with macOS
  brew install bash-completion
  ## or, if running Bash 4.1+
  brew install bash-completion@2
  ## If kubectl is installed via homebrew, this should start working immediately.
  ## If you&#39;ve installed via other means, you may need add the completion to your completion directory
  kubectl completion bash &amp;gt; $(brew --prefix)/etc/bash_completion.d/kubectl


  # Installing bash completion on Linux
  ## If bash-completion is not installed on Linux, please install the &#39;bash-completion&#39; package
  ## via your distribution&#39;s package manager.
  ## Load the kubectl completion code for bash into the current shell
  source &amp;lt;(kubectl completion bash)
  ## Write bash completion code to a file and source if from .bash_profile
  kubectl completion bash &amp;gt; ~/.kube/completion.bash.inc
  printf &amp;quot;
  # Kubectl shell completion
  source &#39;$HOME/.kube/completion.bash.inc&#39;
  &amp;quot; &amp;gt;&amp;gt; $HOME/.bash_profile
  source $HOME/.bash_profile

  # Load the kubectl completion code for zsh[1] into the current shell
  source &amp;lt;(kubectl completion zsh)
  # Set the kubectl completion code for zsh[1] to autoload on startup
  kubectl completion zsh &amp;gt; &amp;quot;${fpath[1]}/_kubectl&amp;quot;

Usage:
  kubectl completion SHELL [options]

Use &amp;quot;kubectl options&amp;quot; for a list of global command-line options (applies to all commands).

按照指南操作即可
配置多个上下文环境
查看当前上下文配置
$ kubectl config get-contexts
CURRENT   NAME       CLUSTER    AUTHINFO           NAMESPACE
*         minikube   minikube   minikube

编辑~/.kube/config文件新增需要的上下文配置即可，配置完成后查看上下文信息
$ kubectl config get-contexts
CURRENT   NAME       CLUSTER    AUTHINFO           NAMESPACE
          kubeadm    kubeadm    kubernetes-admin
*         minikube   minikube   minikube

切换上下文
$ kubectl config use-context kubeadm
Switched to context &amp;quot;kubeadm&amp;quot;.

再次查看，可见切换成功
$ kubectl config get-contexts
CURRENT   NAME       CLUSTER    AUTHINFO           NAMESPACE
*         kubeadm    kubeadm    kubernetes-admin
          minikube   minikube   minikube

Pod相关操作
默认查看default命名空间的Pod
$ kubectl get pod
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-54f57cf6bf-pznmh   1/1     Running   0          52s
nginx-deployment-54f57cf6bf-r9q89   1/1     Running   0          52s

查看所有命名空间的Pod
$ kubectl get pod --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-58cc8c89f4-c297b             1/1     Running   0          10m
kube-system   coredns-58cc8c89f4-spzhj             1/1     Running   0          10m
kube-system   etcd-k8s-master                      1/1     Running   0          9m57s
kube-system   kube-apiserver-k8s-master            1/1     Running   0          10m
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          9m58s
kube-system   kube-flannel-ds-amd64-6fwg5          1/1     Running   0          5m
kube-system   kube-flannel-ds-amd64-6rchg          1/1     Running   0          9m12s
kube-system   kube-flannel-ds-amd64-z4plh          1/1     Running   0          4m54s
kube-system   kube-proxy-cdcs4                     1/1     Running   0          5m
kube-system   kube-proxy-nlmbb                     1/1     Running   0          4m54s
kube-system   kube-proxy-xlb62                     1/1     Running   0          10m
kube-system   kube-scheduler-k8s-master            1/1     Running   0          10m

指定Pod对应的app名称
$ kubectl get pod -l app=nginx
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-54f57cf6bf-pznmh   1/1     Running   0          58s
nginx-deployment-54f57cf6bf-r9q89   1/1     Running   0          58s

查看更多信息
$ kubectl get deployment -o wide
NAME               READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
nginx-deployment   2/2     2            2           64s   nginx        nginx:1.7.9   app=nginx

查看Pod详细信息
$ kubectl describe pod
Name:         nginx-deployment-54f57cf6bf-67llv
Namespace:    default
Priority:     0
Node:         k8s-node1/10.0.2.15
Start Time:   Sun, 20 Oct 2019 10:19:42 +0800
Labels:       app=nginx
              pod-template-hash=54f57cf6bf
Annotations:  &amp;lt;none&amp;gt;
Status:       Running
IP:           172.100.1.3
IPs:
  IP:           172.100.1.3
Controlled By:  ReplicaSet/nginx-deployment-54f57cf6bf
Containers:
  nginx:
    Container ID:   docker://ffd4b4fa3fd4678a61797cfdcba31ea2b0b36459b65a95100484eda4be286b9f
    Image:          nginx:1.7.9
    Image ID:       docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 20 Oct 2019 10:19:45 +0800
    Ready:          True
    Restart Count:  0
    Environment:    &amp;lt;none&amp;gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zjpq9 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-zjpq9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zjpq9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &amp;lt;none&amp;gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age        From                Message
  ----    ------     ----       ----                -------
  Normal  Scheduled  &amp;lt;unknown&amp;gt;  default-scheduler   Successfully assigned default/nginx-deployment-54f57cf6bf-67llv to k8s-node1
  Normal  Pulled     46s        kubelet, k8s-node1  Container image &amp;quot;nginx:1.7.9&amp;quot; already present on machine
  Normal  Created    46s        kubelet, k8s-node1  Created container nginx
  Normal  Started    45s        kubelet, k8s-node1  Started container nginx


Name:         nginx-deployment-54f57cf6bf-r9q89
Namespace:    default
Priority:     0
Node:         k8s-node2/10.0.2.15
Start Time:   Sun, 20 Oct 2019 10:17:24 +0800
Labels:       app=nginx
              pod-template-hash=54f57cf6bf
Annotations:  &amp;lt;none&amp;gt;
Status:       Running
IP:           172.100.2.3
IPs:
  IP:           172.100.2.3
Controlled By:  ReplicaSet/nginx-deployment-54f57cf6bf
Containers:
  nginx:
    Container ID:   docker://61894b3762d723eabe2510284bb107098bdbcb8da4f841eea863a9c7a59c5806
    Image:          nginx:1.7.9
    Image ID:       docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 20 Oct 2019 10:18:08 +0800
    Ready:          True
    Restart Count:  0
    Environment:    &amp;lt;none&amp;gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zjpq9 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-zjpq9:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zjpq9
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &amp;lt;none&amp;gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age        From                Message
  ----    ------     ----       ----                -------
  Normal  Scheduled  &amp;lt;unknown&amp;gt;  default-scheduler   Successfully assigned default/nginx-deployment-54f57cf6bf-r9q89 to k8s-node2
  Normal  Pulling    3m4s       kubelet, k8s-node2  Pulling image &amp;quot;nginx:1.7.9&amp;quot;
  Normal  Pulled     2m24s      kubelet, k8s-node2  Successfully pulled image &amp;quot;nginx:1.7.9&amp;quot;
  Normal  Created    2m22s      kubelet, k8s-node2  Created container nginx
  Normal  Started    2m22s      kubelet, k8s-node2  Started container nginx

Deployment相关操作
部署Deployment
$ kubectl create -f nginx_deployment.yml
deployment.apps/nginx-deployment created

升级Deployment
apply兼有create的语义，故也可直接使用apply创建\部署deployment
$ kubectl apply -f nginx_deployment_scale.yml
deployment.apps/nginx-deployment configured

查看Deployment更多信息
$ kubectl get deployment -o wide
NAME               READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES      SELECTOR
nginx-deployment   2/2     2            2           5m28s   nginx        nginx:1.8   app=nginx

查看详细信息
$ kubectl describe deployments.apps
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 20 Oct 2019 10:17:24 +0800
Labels:                 &amp;lt;none&amp;gt;
Annotations:            deployment.kubernetes.io/revision: 3
                        kubectl.kubernetes.io/last-applied-configuration:
                          {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Deployment&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;nginx-deployment&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replica...
Selector:               app=nginx
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  &amp;lt;none&amp;gt;
NewReplicaSet:   nginx-deployment-56f8998dbc (2/2 replicas created)
Events:
  Type    Reason             Age                  From                   Message
  ----    ------             ----                 ----                   -------
  Normal  ScalingReplicaSet  22m                  deployment-controller  Scaled up replica set nginx-deployment-54f57cf6bf to 2
  Normal  ScalingReplicaSet  18m                  deployment-controller  Scaled up replica set nginx-deployment-9f46bb5 to 1
  Normal  ScalingReplicaSet  17m                  deployment-controller  Scaled down replica set nginx-deployment-54f57cf6bf to 1
  Normal  ScalingReplicaSet  17m                  deployment-controller  Scaled up replica set nginx-deployment-9f46bb5 to 2
  Normal  ScalingReplicaSet  16m                  deployment-controller  Scaled down replica set nginx-deployment-54f57cf6bf to 0
  Normal  ScalingReplicaSet  15m                  deployment-controller  Scaled up replica set nginx-deployment-9f46bb5 to 4
  Normal  ScalingReplicaSet  12m                  deployment-controller  Scaled down replica set nginx-deployment-9f46bb5 to 3
  Normal  ScalingReplicaSet  6m25s                deployment-controller  Scaled up replica set nginx-deployment-56f8998dbc to 1
  Normal  ScalingReplicaSet  5m43s                deployment-controller  Scaled down replica set nginx-deployment-9f46bb5 to 2
  Normal  ScalingReplicaSet  63s (x6 over 5m43s)  deployment-controller  (combined from similar events): Scaled down replica set nginx-deployment-56f8998dbc to 2

Node相关
查看Node
$ kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   24m   v1.16.2
k8s-node1    Ready    &amp;lt;none&amp;gt;   18m   v1.16.2
k8s-node2    Ready    &amp;lt;none&amp;gt;   18m   v1.16.2

查看更多信息
$ kubectl get nodes -o wide
NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
k8s-master   Ready    master   24m   v1.16.2   10.0.2.15     &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.12.2.el7.x86_64   docker://18.9.8
k8s-node1    Ready    &amp;lt;none&amp;gt;   18m   v1.16.2   10.0.2.15     &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.12.2.el7.x86_64   docker://18.9.8
k8s-node2    Ready    &amp;lt;none&amp;gt;   18m   v1.16.2   10.0.2.15     &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-957.12.2.el7.x86_64   docker://18.9.8

查看标签
$ kubectl get nodes --show-labels
NAME         STATUS   ROLES    AGE   VERSION   LABELS
k8s-master   Ready    master   24m   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux,node-role.kubernetes.io/master=
k8s-node1    Ready    &amp;lt;none&amp;gt;   18m   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node1,kubernetes.io/os=linux
k8s-node2    Ready    &amp;lt;none&amp;gt;   18m   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node2,kubernetes.io/os=linux

设置角色标签
$ kubectl label nodes k8s-node1 node-role.kubernetes.io/worker=
node/k8s-node1 labeled

查看详细信息
$ kubectl describe nodes
Name:               k8s-master
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-master
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VtepMAC&amp;quot;:&amp;quot;32:38:c5:18:f0:ed&amp;quot;}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.0.2.15
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 20 Oct 2019 10:05:02 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 20 Oct 2019 10:27:46 +0800   Sun, 20 Oct 2019 10:05:02 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 20 Oct 2019 10:27:46 +0800   Sun, 20 Oct 2019 10:05:02 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 20 Oct 2019 10:27:46 +0800   Sun, 20 Oct 2019 10:05:02 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 20 Oct 2019 10:27:46 +0800   Sun, 20 Oct 2019 10:07:42 +0800   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.0.2.15
  Hostname:    k8s-master
Capacity:
 cpu:                2
 ephemeral-storage:  41921540Ki
 hugepages-2Mi:      0
 memory:             1882144Ki
 pods:               110
Allocatable:
 cpu:                2
 ephemeral-storage:  38634891201
 hugepages-2Mi:      0
 memory:             1779744Ki
 pods:               110
System Info:
 Machine ID:                 d74c50274729456aad2a6198de9863b3
 System UUID:                D74C5027-4729-456A-AD2A-6198DE9863B3
 Boot ID:                    fb2ea7bd-afb2-4051-afb7-78d9f59ec6f6
 Kernel Version:             3.10.0-957.12.2.el7.x86_64
 OS Image:                   CentOS Linux 7 (Core)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://18.9.8
 Kubelet Version:            v1.16.2
 Kube-Proxy Version:         v1.16.2
PodCIDR:                     172.100.0.0/24
PodCIDRs:                    172.100.0.0/24
Non-terminated Pods:         (8 in total)
  Namespace                  Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                  ----                                  ------------  ----------  ---------------  -------------  ---
  kube-system                coredns-58cc8c89f4-c297b              100m (5%)     0 (0%)      70Mi (4%)        170Mi (9%)     23m
  kube-system                coredns-58cc8c89f4-spzhj              100m (5%)     0 (0%)      70Mi (4%)        170Mi (9%)     23m
  kube-system                etcd-k8s-master                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m
  kube-system                kube-apiserver-k8s-master             250m (12%)    0 (0%)      0 (0%)           0 (0%)         22m
  kube-system                kube-controller-manager-k8s-master    200m (10%)    0 (0%)      0 (0%)           0 (0%)         22m
  kube-system                kube-flannel-ds-amd64-6rchg           100m (5%)     100m (5%)   50Mi (2%)        50Mi (2%)      21m
  kube-system                kube-proxy-xlb62                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m
  kube-system                kube-scheduler-k8s-master             100m (5%)     0 (0%)      0 (0%)           0 (0%)         22m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                850m (42%)   100m (5%)
  memory             190Mi (10%)  390Mi (22%)
  ephemeral-storage  0 (0%)       0 (0%)
Events:
  Type    Reason                   Age                From                    Message
  ----    ------                   ----               ----                    -------
  Normal  NodeHasSufficientMemory  23m (x8 over 23m)  kubelet, k8s-master     Node k8s-master status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    23m (x8 over 23m)  kubelet, k8s-master     Node k8s-master status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     23m (x7 over 23m)  kubelet, k8s-master     Node k8s-master status is now: NodeHasSufficientPID
  Normal  Starting                 23m                kube-proxy, k8s-master  Starting kube-proxy.


Name:               k8s-node1
Roles:              &amp;lt;none&amp;gt;
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-node1
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VtepMAC&amp;quot;:&amp;quot;16:5c:40:b1:14:b6&amp;quot;}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.0.2.15
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 20 Oct 2019 10:11:03 +0800
Taints:             &amp;lt;none&amp;gt;
Unschedulable:      false
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 20 Oct 2019 10:28:08 +0800   Sun, 20 Oct 2019 10:11:03 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 20 Oct 2019 10:28:08 +0800   Sun, 20 Oct 2019 10:11:03 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 20 Oct 2019 10:28:08 +0800   Sun, 20 Oct 2019 10:11:03 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 20 Oct 2019 10:28:08 +0800   Sun, 20 Oct 2019 10:11:34 +0800   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.0.2.15
  Hostname:    k8s-node1
Capacity:
 cpu:                1
 ephemeral-storage:  41921540Ki
 hugepages-2Mi:      0
 memory:             1882296Ki
 pods:               110
Allocatable:
 cpu:                1
 ephemeral-storage:  38634891201
 hugepages-2Mi:      0
 memory:             1779896Ki
 pods:               110
System Info:
 Machine ID:                 5d806dde9b0444bdb5c9800a228267d8
 System UUID:                5D806DDE-9B04-44BD-B5C9-800A228267D8
 Boot ID:                    e6b7222c-686c-49cb-8e56-9bb263852d36
 Kernel Version:             3.10.0-957.12.2.el7.x86_64
 OS Image:                   CentOS Linux 7 (Core)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://18.9.8
 Kubelet Version:            v1.16.2
 Kube-Proxy Version:         v1.16.2
PodCIDR:                     172.100.1.0/24
PodCIDRs:                    172.100.1.0/24
Non-terminated Pods:         (4 in total)
  Namespace                  Name                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                  ----                              ------------  ----------  ---------------  -------------  ---
  default                    nginx-deployment-9f46bb5-7vlb7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m53s
  default                    nginx-deployment-9f46bb5-c7czx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m35s
  kube-system                kube-flannel-ds-amd64-6fwg5       100m (10%)    100m (10%)  50Mi (2%)        50Mi (2%)      17m
  kube-system                kube-proxy-cdcs4                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                100m (10%)  100m (10%)
  memory             50Mi (2%)   50Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
Events:
  Type    Reason                   Age   From                   Message
  ----    ------                   ----  ----                   -------
  Normal  Starting                 17m   kubelet, k8s-node1     Starting kubelet.
  Normal  NodeHasSufficientMemory  17m   kubelet, k8s-node1     Node k8s-node1 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m   kubelet, k8s-node1     Node k8s-node1 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m   kubelet, k8s-node1     Node k8s-node1 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  17m   kubelet, k8s-node1     Updated Node Allocatable limit across pods
  Normal  Starting                 17m   kube-proxy, k8s-node1  Starting kube-proxy.
  Normal  NodeReady                16m   kubelet, k8s-node1     Node k8s-node1 status is now: NodeReady


Name:               k8s-node2
Roles:              &amp;lt;none&amp;gt;
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-node2
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {&amp;quot;VtepMAC&amp;quot;:&amp;quot;76:49:24:1f:e5:d6&amp;quot;}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.0.2.15
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 20 Oct 2019 10:11:09 +0800
Taints:             &amp;lt;none&amp;gt;
Unschedulable:      false
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 20 Oct 2019 10:27:42 +0800   Sun, 20 Oct 2019 10:11:09 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 20 Oct 2019 10:27:42 +0800   Sun, 20 Oct 2019 10:11:09 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 20 Oct 2019 10:27:42 +0800   Sun, 20 Oct 2019 10:11:09 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 20 Oct 2019 10:27:42 +0800   Sun, 20 Oct 2019 10:11:40 +0800   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.0.2.15
  Hostname:    k8s-node2
Capacity:
 cpu:                1
 ephemeral-storage:  41921540Ki
 hugepages-2Mi:      0
 memory:             1882296Ki
 pods:               110
Allocatable:
 cpu:                1
 ephemeral-storage:  38634891201
 hugepages-2Mi:      0
 memory:             1779896Ki
 pods:               110
System Info:
 Machine ID:                 7c59ba12638e4fdfa20cfeda0aa2fda1
 System UUID:                7C59BA12-638E-4FDF-A20C-FEDA0AA2FDA1
 Boot ID:                    e402f75c-42e7-454d-aaca-f96853091c15
 Kernel Version:             3.10.0-957.12.2.el7.x86_64
 OS Image:                   CentOS Linux 7 (Core)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://18.9.8
 Kubelet Version:            v1.16.2
 Kube-Proxy Version:         v1.16.2
PodCIDR:                     172.100.2.0/24
PodCIDRs:                    172.100.2.0/24
Non-terminated Pods:         (3 in total)
  Namespace                  Name                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                  ----                              ------------  ----------  ---------------  -------------  ---
  default                    nginx-deployment-9f46bb5-mv4b7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m52s
  kube-system                kube-flannel-ds-amd64-z4plh       100m (10%)    100m (10%)  50Mi (2%)        50Mi (2%)      17m
  kube-system                kube-proxy-nlmbb                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                100m (10%)  100m (10%)
  memory             50Mi (2%)   50Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
Events:
  Type    Reason                   Age   From                   Message
  ----    ------                   ----  ----                   -------
  Normal  Starting                 17m   kubelet, k8s-node2     Starting kubelet.
  Normal  NodeHasSufficientMemory  17m   kubelet, k8s-node2     Node k8s-node2 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m   kubelet, k8s-node2     Node k8s-node2 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m   kubelet, k8s-node2     Node k8s-node2 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  17m   kubelet, k8s-node2     Updated Node Allocatable limit across pods
  Normal  Starting                 17m   kube-proxy, k8s-node2  Starting kube-proxy.
  Normal  NodeReady                16m   kubelet, k8s-node2     Node k8s-node2 status is now: NodeReady

服务伸缩
scale命令方式
$ kubectl scale --replicas=6 deployment/nginx-deployment
deployment.apps/nginx-deployment scaled

查看副本数
$ kubectl get replicaset
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-54f57cf6bf   0         0         0       21m
nginx-deployment-56f8998dbc   6         6         3       5m7s
nginx-deployment-9f46bb5      0         0         0       16m


                                        </div>
                                        
                                            <a class="btn btn-text" href="https://fjiayang.github.io//post/kubernetes-ji-chu-ming-ling-xue-xi">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://fjiayang.github.io//post/kubernetes-ji-qun-da-jian">
                        Kubernetes 集群搭建
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2019-10-20</time>
                    
                        <a href="https://fjiayang.github.io//tag/yb4HlP89R" class="post-tag i-tag
                            i-tag-success">
            #Kubernetes
        </a>
                        
                        <a href="https://fjiayang.github.io//tag/kTPZcu1_NZ" class="post-tag i-tag
                            i-tag-banana">
            #Docker
        </a>
                        
                        <a href="https://fjiayang.github.io//tag/rjmWDGdyvm" class="post-tag i-tag
                            i-tag-primary">
            #集群
        </a>
                        
                </div>
                <div class="post-article">
                    
                        <a href="https://fjiayang.github.io//post/kubernetes-ji-qun-da-jian" class="post-feature-image" style="background-image:url(https://s2.ax1x.com/2019/11/27/QCRTo9.jpg) ">
                        </a>
                        
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            搭建方法分类
如果是搭建Kubernetes的学习环境，则可以直接使用minikube快速搭建单节点的Kubernetes环境，官方推荐使用kubeadm搭建Kubernetes集群生产环境
但kubeadm需要连接谷歌容器仓库获取镜像，在网络受限的情况下无法搭建成功，故有两种搭建方法：

服务器使用代理，代理所有的http和https连接，可以自由访问任意地址
使用阿里云镜像地址

本次采用第二种方式进行搭建
环境准备
本次宿主机为CentOS 7，结合3台virtual box虚拟机模拟集群环境，虚拟机使用Vagrant进行管理，vagrant安装比较简单，此处不再赘述
宿主机配置：

CPU：4C
RAM：8G

虚拟机配置
由于网络受限，vagrant默认拉取的操作系统文件地址不可访问，需要自行从官方下载镜像或者使用代理下载镜像，此处对此操作也不再赘述
三台虚拟机配置文件
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.require_version &amp;quot;&amp;gt;= 1.6.0&amp;quot;

boxes = [
    {
        :name =&amp;gt; &amp;quot;k8s-master&amp;quot;,
        :eth1 =&amp;gt; &amp;quot;192.168.205.120&amp;quot;,
        :mem =&amp;gt; &amp;quot;2048&amp;quot;,
        :cpu =&amp;gt; &amp;quot;2&amp;quot;
    },
    {
        :name =&amp;gt; &amp;quot;k8s-node1&amp;quot;,
        :eth1 =&amp;gt; &amp;quot;192.168.205.121&amp;quot;,
        :mem =&amp;gt; &amp;quot;2048&amp;quot;,
        :cpu =&amp;gt; &amp;quot;1&amp;quot;
    },
    {
        :name =&amp;gt; &amp;quot;k8s-node2&amp;quot;,
        :eth1 =&amp;gt; &amp;quot;192.168.205.122&amp;quot;,
        :mem =&amp;gt; &amp;quot;2048&amp;quot;,
        :cpu =&amp;gt; &amp;quot;1&amp;quot;
    }

]

Vagrant.configure(2) do |config|

  config.vm.box = &amp;quot;centos/7&amp;quot;
  boxes.each do |opts|
    config.vm.define opts[:name] do |config|
      config.vm.hostname = opts[:name]
      config.vm.provider &amp;quot;vmware_fusion&amp;quot; do |v|
        v.vmx[&amp;quot;memsize&amp;quot;] = opts[:mem]
        v.vmx[&amp;quot;numvcpus&amp;quot;] = opts[:cpu]
      end
      config.vm.provider &amp;quot;virtualbox&amp;quot; do |v|
        v.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--memory&amp;quot;, opts[:mem]]
        v.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--cpus&amp;quot;, opts[:cpu]]
      end
      config.vm.network :private_network, ip: opts[:eth1]
    end
  end
  config.vm.provision &amp;quot;shell&amp;quot;, privileged: true, path: &amp;quot;./setup.sh&amp;quot;
end

所有虚拟机启动后都会执行./setup.sh脚本配置基础环境，脚本内容如下
#/bin/sh

sudo yum install -y vim telnet bind-utils wget yum-utils

sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
sudo yum -y install docker-ce-18.09.8 docker-ce-cli-18.09.8 containerd.io

if [ ! $(getent group docker) ];
then 
    sudo groupadd docker;
else
    echo &amp;quot;docker user group already exists&amp;quot;
fi

sudo gpasswd -a $USER docker
sudo systemctl restart docker

# open password auth for backup if ssh key doesn&#39;t work, bydefault, username=vagrant password=vagrant
sudo sed -i &#39;s/PasswordAuthentication no/PasswordAuthentication yes/g&#39; /etc/ssh/sshd_config
sudo systemctl restart sshd

sudomkdir /etc/yum.repos.d/bak &amp;amp;&amp;amp; mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak
sudowget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.cloud.tencent.com/repo/centos7_base.repo
sudowget -O /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repo

yum clean all &amp;amp;&amp;amp; yum makecache

sudo bash -c &#39;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF&#39;

sudo wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
{
  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://clayphwh.mirror.aliyuncs.com&amp;quot;]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker

sudo setenforce 0

sudo yum install -y kubelet kubeadm kubectl

sudo bash -c &#39;cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward=1
EOF&#39;
sudo sysctl --system

sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo swapoff -a

sudo systemctl enable docker.service
sudo systemctl enable kubelet.service

脚本内已将K8S镜像均配置为阿里云镜像仓库，并配置了阿里云容器加速服务地址，更便于在国内拉取镜像
启动虚拟机
使用命令启动虚拟机
$ vagrant up

启动需要较长时间，若出现大量超时也请耐心等待
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.aliyun.com
 * extras: mirrors.163.com
 * updates: mirrors.163.com
http://vault.centos.org/7.1.1503/os/x86_64/repodata/0e6e90965f55146ba5025ea450f822d1bb0267d0299ef64dd4365825e6bad995-c7-x86_64-comps.xml.gz: [Errno 12] Timeout on http://vault.centos.org/7.1.1503/os/x86_64/repodata/0e6e90965f55146ba5025ea450f822d1bb0267d0299ef64dd4365825e6bad995-c7-x86_64-comps.xml.gz: (28, &#39;Operation too slow. Less than 1000 bytes/sec transferred the last 30 seconds&#39;)
Trying other mirror.
http://vault.centos.org/7.1.1503/updates/x86_64/repodata/93b71f445d2ec2138d28152612f4fb29c8e76ee31f2666b964d88249b4e0a955-primary.sqlite.bz2: [Errno 12] Timeout on http://vault.centos.org/7.1.1503/updates/x86_64/repodata/93b71f445d2ec2138d28152612f4fb29c8e76ee31f2666b964d88249b4e0a955-primary.sqlite.bz2: (28, &#39;Operation too slow. Less than 1000 bytes/sec transferred the last 30 seconds&#39;)
Trying other mirror.
http://vault.centos.org/7.2.1511/os/x86_64/repodata/c6411f1cc8a000ed2b651b49134631d279abba1ec1f78e5dcca79a52d8c1eada-primary.sqlite.bz2: [Errno 12] Timeout on http://vault.centos.org/7.2.1511/os/x86_64/repodata/c6411f1cc8a000ed2b651b49134631d279abba1ec1f78e5dcca79a52d8c1eada-primary.sqlite.bz2: (28, &#39;Operation too slow. Less than 1000 bytes/sec transferred the last 30 seconds&#39;)
Trying other mirror.
...

启动完成后通过命令查看虚拟机状态
$ vagrant status
Current machine states:

k8s-master                running (virtualbox)
k8s-node1                 running (virtualbox)
k8s-node2                 running (virtualbox)

This environment represents multiple VMs. The VMs are all listed
above with their current state. For more information about a specific
VM, run `vagrant status NAME`.

K8S master节点初始化
进入主节点，查看环境是否已经配置完成
$ vagrant ssh k8s-master

执行以下三条语句查看是否有预期输出
[vagrant@k8s-master ~]$ sudo which kubeadm
/bin/kubeadm
[vagrant@k8s-master ~]$ sudo which kubelet
/bin/kubelet
[vagrant@k8s-master ~]$ sudo which kubectl
/bin/kubectl
[vagrant@k8s-master ~]$ sudo docker version
Client:
 Version:           18.09.8
 API version:       1.39
 Go version:        go1.10.8
 Git commit:        0dd43dd87f
 Built:             Wed Jul 17 17:40:31 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          18.09.8
  API version:      1.39 (minimum version 1.12)
  Go version:       go1.10.8
  Git commit:       0dd43dd
  Built:            Wed Jul 17 17:10:42 2019
  OS/Arch:          linux/amd64
  Experimental:     false

使用初始化命令，并指定拉取镜像地址为阿里云地址，同时指定容器网络地址和注册中心广播地址
$ sudo kubeadm init --pod-network-cidr 172.100.0.0/16 --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address 192.168.205.120

输出如下即初始化完成
$ sudo kubeadm init --pod-network-cidr 172.100.0.0/16 --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address 192.168.205.120
W1020 01:31:27.560129   20473 version.go:101] could not fetch a `Kubernetes`version from the internet: unable to get URL &amp;quot;https://dl.k8s.io/release/stable-1.txt&amp;quot;: Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W1020 01:31:27.560713   20473 version.go:102] falling back to the local client version: v1.16.2
[init] Using `Kubernetes`version: v1.16.2
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected &amp;quot;cgroupfs&amp;quot; as the Docker cgroup driver. The recommended driver is &amp;quot;systemd&amp;quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Pulling images required for setting up a `Kubernetes`cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder &amp;quot;/etc/kubernetes/pki&amp;quot;
[certs] Generating &amp;quot;ca&amp;quot; certificate and key
[certs] Generating &amp;quot;apiserver&amp;quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master `kubernetes`kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.205.120]
[certs] Generating &amp;quot;apiserver-kubelet-client&amp;quot; certificate and key
[certs] Generating &amp;quot;front-proxy-ca&amp;quot; certificate and key
[certs] Generating &amp;quot;front-proxy-client&amp;quot; certificate and key
[certs] Generating &amp;quot;etcd/ca&amp;quot; certificate and key
[certs] Generating &amp;quot;etcd/server&amp;quot; certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.205.120 127.0.0.1 ::1]
[certs] Generating &amp;quot;etcd/peer&amp;quot; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.205.120 127.0.0.1 ::1]
[certs] Generating &amp;quot;etcd/healthcheck-client&amp;quot; certificate and key
[certs] Generating &amp;quot;apiserver-etcd-client&amp;quot; certificate and key
[certs] Generating &amp;quot;sa&amp;quot; key and public key
[kubeconfig] Using kubeconfig folder &amp;quot;/etc/kubernetes&amp;quot;
[kubeconfig] Writing &amp;quot;admin.conf&amp;quot; kubeconfig file
[kubeconfig] Writing &amp;quot;kubelet.conf&amp;quot; kubeconfig file
[kubeconfig] Writing &amp;quot;controller-manager.conf&amp;quot; kubeconfig file
[kubeconfig] Writing &amp;quot;scheduler.conf&amp;quot; kubeconfig file
[control-plane] Using manifest folder &amp;quot;/etc/kubernetes/manifests&amp;quot;
[control-plane] Creating static Pod manifest for &amp;quot;kube-apiserver&amp;quot;
[control-plane] Creating static Pod manifest for &amp;quot;kube-controller-manager&amp;quot;
[control-plane] Creating static Pod manifest for &amp;quot;kube-scheduler&amp;quot;
[etcd] Creating static Pod manifest for local etcd in &amp;quot;/etc/kubernetes/manifests&amp;quot;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;. This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 59.511578 seconds
[upload-config] Storing the configuration used in ConfigMap &amp;quot;kubeadm-config&amp;quot; in the &amp;quot;kube-system&amp;quot; Namespace
[kubelet] Creating a ConfigMap &amp;quot;kubelet-config-1.16&amp;quot; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the label &amp;quot;node-role.kubernetes.io/master=&#39;&#39;&amp;quot;
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: rod570.p67pymzbil4m6u8d
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &amp;quot;cluster-info&amp;quot; ConfigMap in the &amp;quot;kube-public&amp;quot; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your `Kubernetes`control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.205.120:6443 --token rod570.p67pymzbil4m6u8d \
    --discovery-token-ca-cert-hash sha256:8f4cb9d555c78d58befeb3cfa3f7537989aa599e53e4f1bae929d8cc7afd1476

kubectl 配置
拷贝kubectl配置
$ rm -rf ~/.kube
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

若不配置直接使用则报无法连接错误
[vagrant@k8s-master ~]$ kubectl get pod --all-namespaces
The connection to the server localhost:8080 was refused - did you specify the right host or port?

此时使用kubectl可查看k8s容器运行情况，输出如下则为初始化成功，启动由于未配置网络插件，故coredns一直处于Pending状态
[vagrant@k8s-master ~]$ kubectl get pod --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-58cc8c89f4-c297b             0/1     Pending   0          81s
kube-system   coredns-58cc8c89f4-spzhj             0/1     Pending   0          81s
kube-system   etcd-k8s-master                      1/1     Running   0          39s
kube-system   kube-apiserver-k8s-master            1/1     Running   0          49s
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          40s
kube-system   kube-proxy-xlb62                     1/1     Running   0          81s
kube-system   kube-scheduler-k8s-master            1/1     Running   0          51s

安装网络插件
网络插件有多个可选，可通过官网查看，此处选择安装flannel CNI网络插件

For flannel to work correctly, you must pass --pod-network-cidr=10.244.0.0/16 to kubeadm init.
Set /proc/sys/net/bridge/bridge-nf-call-iptables to 1 by running sysctl net.bridge.bridge-nf-call-iptables=1 to pass bridged IPv4 traffic to iptables’ chains. This is a requirement for some CNI plugins to work, for more information please see here.
Make sure that your firewall rules allow UDP ports 8285 and 8472 traffic for all hosts participating in the overlay network. see here .
Note that flannel works on amd64, arm, arm64, ppc64le and s390x under Linux. Windows (amd64) is claimed as supported in v0.11.0 but the usage is undocumented.
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

For more information about flannel, see the CoreOS flannel repository on GitHub .

相关的网络配置已在脚本中写入，此时直接执行yml脚本即可
$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

再次查看容器运行情况，所有容器均已运行，则master节点配置完成
[vagrant@k8s-master ~]$ kubectl get pod --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-58cc8c89f4-c297b             1/1     Running   0          5m17s
kube-system   coredns-58cc8c89f4-spzhj             1/1     Running   0          5m17s
kube-system   etcd-k8s-master                      1/1     Running   0          4m35s
kube-system   kube-apiserver-k8s-master            1/1     Running   0          4m45s
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          4m36s
kube-system   kube-flannel-ds-amd64-6rchg          1/1     Running   0          3m50s
kube-system   kube-proxy-xlb62                     1/1     Running   0          5m17s
kube-system   kube-scheduler-k8s-master            1/1     Running   0          4m47s

配置worker节点
以k8s-node1为例，执行master初始化时提供的脚本即可加入k8s集群，输出如下则加入成功
[vagrant@k8s-node1 ~]$ sudo kubeadm join 192.168.205.120:6443 --token 8ry5oo.y48ksgurn103zq4h \
&amp;gt;     --discovery-token-ca-cert-hash sha256:ead07352591500c2cfe3321bf87d2e068790e16f2a7e0cc23541c864d24006d4
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected &amp;quot;cgroupfs&amp;quot; as the Docker cgroup driver. The recommended driver is &amp;quot;systemd&amp;quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;
[kubelet-start] Downloading configuration for the kubelet from the &amp;quot;kubelet-config-1.16&amp;quot; ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;
[kubelet-start] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.

稍等片刻进入master节点查看容器运行情况，会发现多了几个网络容器，属于预期情况
[vagrant@k8s-master ~]$ kubectl get pod --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-58cc8c89f4-c297b             1/1     Running   0          6m28s
kube-system   coredns-58cc8c89f4-spzhj             1/1     Running   0          6m28s
kube-system   etcd-k8s-master                      1/1     Running   0          5m46s
kube-system   kube-apiserver-k8s-master            1/1     Running   0          5m56s
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          5m47s
kube-system   kube-flannel-ds-amd64-6fwg5          1/1     Running   0          49s
kube-system   kube-flannel-ds-amd64-6rchg          1/1     Running   0          5m1s
kube-system   kube-flannel-ds-amd64-z4plh          1/1     Running   0          43s
kube-system   kube-proxy-cdcs4                     1/1     Running   0          49s
kube-system   kube-proxy-nlmbb                     1/1     Running   0          43s
kube-system   kube-proxy-xlb62                     1/1     Running   0          6m28s
kube-system   kube-scheduler-k8s-master            1/1     Running   0          5m58s

集群验证
在master节点中启动nginx容器测试集群情况
$ kubectl create deployment nginx --image=nginx

稍等片刻，输出如下则集群正常运行，k8s集群配置完成
[vagrant@k8s-master ~]$ kubectl get pod --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
default       nginx-86c57db685-2gkxm               1/1     Running   0          58s
kube-system   coredns-58cc8c89f4-c297b             1/1     Running   0          7m46s
kube-system   coredns-58cc8c89f4-spzhj             1/1     Running   0          7m46s
kube-system   etcd-k8s-master                      1/1     Running   0          7m4s
kube-system   kube-apiserver-k8s-master            1/1     Running   0          7m14s
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          7m5s
kube-system   kube-flannel-ds-amd64-6fwg5          1/1     Running   0          2m7s
kube-system   kube-flannel-ds-amd64-6rchg          1/1     Running   0          6m19s
kube-system   kube-flannel-ds-amd64-z4plh          1/1     Running   0          2m1s
kube-system   kube-proxy-cdcs4                     1/1     Running   0          2m7s
kube-system   kube-proxy-nlmbb                     1/1     Running   0          2m1s
kube-system   kube-proxy-xlb62                     1/1     Running   0          7m46s
kube-system   kube-scheduler-k8s-master            1/1     Running   0          7m16s


                                        </div>
                                        
                                            <a class="btn btn-text" href="https://fjiayang.github.io//post/kubernetes-ji-qun-da-jian">Read More ~</a>
                            </div>
                </div>
            </article>
            
                <!-- 翻页 -->
                
                </div>
                <!--  -->
                <div class="main-container-middle"></div>
                <!--  -->
                <div id="sidebar" class="main-container-right">

                    <!-- 个人信息 -->
                    
    <div class="id_card i-card">
        <div class="id_card-avatar" style="background-image: url(https://fjiayang.github.io//images/avatar.png?v=1575360921810)">
        </div>
        <h1 class="id_card-title">
            F嘉阳
        </h1>
        <h2 class="id_card-description">
            温故而知新
        </h2>
        <!--  -->
        <div class="id_card-sns">
            <!-- github -->
            
                    <!-- twitter -->
                    
                            <!-- weibo -->
                            
                                    <!-- facebook -->
                                    

        </div>
    </div>
    

                        <!-- 公告栏 -->
                        

                </div>
            </div>



            <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://fjiayang.github.io//atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
    <script>
        $('#sidebar').stickySidebar({
            topSpacing: 80,
            // bottomSpacing: 60
        });
    </script>
</body>

</html>